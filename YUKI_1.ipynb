{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMi8KET/f5jKy0HUskHrcZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuki020936/Option-NN/blob/main/YUKI_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ-Nw6HBRyQe",
        "outputId": "17ac6a93-597d-48ab-e0cf-a5da218dc3b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on_gpu:True,CV:True,start date:19960101,root:.\n",
            "To reproduce AAAI Gated-neural-networks-for-option-pricing paper results\n",
            "../20200316_32batchsize_autoLR_autoCyclen_1500epochsize_0.0625MtoBratio_0.99994gamma_0_1seed_residual_test_test\n",
            "True\n",
            "True\n",
            "epoch 100, train_loss: 252.780, train_mse: 8996418.786, train_mape: 37.776, test_loss:2393.175, test_mse: 12358508.273, test_mape: 46.612\n",
            "epoch 200, train_loss: 261.634, train_mse: 11346283.133, train_mape: 38.910, test_loss:1290.032, test_mse: 16374424.177, test_mape: 52.918\n",
            "epoch 300, train_loss: 164.784, train_mse: 5147004.337, train_mape: 24.662, test_loss:791.320, test_mse: 7595940.474, test_mape: 35.633\n",
            "epoch 400, train_loss: 91.738, train_mse: 1865232.822, train_mape: 14.343, test_loss:420.195, test_mse: 2783171.209, test_mape: 21.337\n",
            "epoch 500, train_loss: 72.208, train_mse: 520318.831, train_mape: 7.343, test_loss:205.682, test_mse: 788367.934, test_mape: 11.224\n",
            "epoch 600, train_loss: 40.280, train_mse: 132130.011, train_mape: 3.614, test_loss:90.384, test_mse: 200727.375, test_mape: 5.633\n",
            "epoch 700, train_loss: 6.939, train_mse: 26480.249, train_mape: 1.597, test_loss:41.215, test_mse: 40209.438, test_mape: 2.515\n",
            "epoch 800, train_loss: 4.284, train_mse: 5642.934, train_mape: 0.736, test_loss:17.513, test_mse: 8519.447, test_mape: 1.179\n",
            "epoch 900, train_loss: 4.267, train_mse: 940.401, train_mape: 0.328, test_loss:7.681, test_mse: 1390.312, test_mape: 0.520\n",
            "epoch 1000, train_loss: 2.857, train_mse: 146.295, train_mape: 0.175, test_loss:3.444, test_mse: 202.040, test_mape: 0.241\n",
            "epoch 1100, train_loss: 0.661, train_mse: 27.200, train_mape: 0.115, test_loss:1.591, test_mse: 31.444, test_mape: 0.129\n",
            "epoch 1200, train_loss: 0.453, train_mse: 12.682, train_mape: 0.096, test_loss:0.905, test_mse: 14.238, test_mape: 0.099\n",
            "epoch 1300, train_loss: 0.338, train_mse: 7.947, train_mape: 0.082, test_loss:0.620, test_mse: 8.309, test_mape: 0.080\n",
            "epoch 1400, train_loss: 0.562, train_mse: 5.446, train_mape: 0.066, test_loss:0.509, test_mse: 7.287, test_mape: 0.076\n",
            "epoch 1499, train_loss: 0.323, train_mse: 1.916, train_mape: 0.035, test_loss:0.303, test_mse: 1.667, test_mape: 0.035\n",
            "epoch 100, train_loss: 151.267, train_mse: 15592800.063, train_mape: 47.976, test_loss:2103.573, test_mse: 22090817.224, test_mape: 62.309\n",
            "epoch 200, train_loss: 6288.020, train_mse: 13329885.350, train_mape: 42.607, test_loss:1457.359, test_mse: 19320801.802, test_mape: 57.885\n"
          ]
        }
      ],
      "source": [
        "import pdb #debug\n",
        "import numpy as np\n",
        "import os\n",
        "from time import sleep #sleep(?)暫停?秒\n",
        "import signal #處理訊號（signal），這些訊號是作業系統在特定情況下發送給程式的事件\n",
        "import sys\n",
        "import queue #list(q.queue)\n",
        "import shutil\n",
        "import psutil\n",
        "from multiprocessing.connection import wait\n",
        "import torch.multiprocessing as mp\n",
        "import torch\n",
        "from argparse import ArgumentParser\n",
        "from dataset import DataProcessor\n",
        "from utils import clear_files\n",
        "from visulization import plot_model_avg_loss\n",
        "import config\n",
        "from train import TrainEngine\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import time\n",
        "\n",
        "#torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "def single_running_worker():\n",
        "    try:\n",
        "        while True:\n",
        "            # run model\n",
        "            test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape = config.train_engine(*next(config.daily_gen)(0))\n",
        "            print(f\"date: {test_date}, train_mse: {train_mse:.3f}, train_mape: {train_mape:.3f}, test_mse: {test_mse:.3f}, test_mape: {test_mape:.3f}\",flush=True)\n",
        "            # record and update\n",
        "            for v, k in zip([test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape], config.model_loss_list_dict):\n",
        "                config.model_loss_list_dict[k] += [v]\n",
        "            # plot_model_avg_loss(config.result_folder, config.args.model_type, config.model_loss_list_dict)\n",
        "            end_time = time.time()\n",
        "            print(f\"執行時間：{end_time - start_time:.3f} 秒\")\n",
        "    except StopIteration:\n",
        "        pass  # EOF of generator, fine\n",
        "\n",
        "def multi_running_check_in(rank):\n",
        "    # rank is to allocate particular device\n",
        "    # torch.distributed.init_process_group(\"nccl\", rank=rank, world_size=torch.cuda.device_count()))\n",
        "    try:\n",
        "        q = mp.Queue()\n",
        "        p = mp.Process(target=multi_running_job, args=(next(config.daily_gen)(rank), q, config.train_engine))\n",
        "    except StopIteration:\n",
        "        config.procs_info['EOF'] = True\n",
        "        return # do nothing. that is, no new process will be allocated afterwards.\n",
        "\n",
        "    # if we don't update these process info before create new child process, parent process may receive SIGCHLD signal while config.procs_info['process_fd_table'] does not include this new process. hence parent process fail to wait this child process. however, it (child process exits immediately) rarely happens.\n",
        "    p.start()\n",
        "    config.procs_info['loss_queues'].append(q)\n",
        "    config.procs_info['process_list'].append(p)\n",
        "    config.procs_info['process_fd_table'].append(p.sentinel)\n",
        "\n",
        "def multi_running_job(daily_materials, loss_queue, train_engine):\n",
        "    # register signal as we hope SIGINT signal can only be processed by parent process\n",
        "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
        "    # run model\n",
        "    test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape = train_engine(*daily_materials)\n",
        "    print(f\"[process {os.getpid()}] date: {test_date}, train_mse: {train_mse:.3f}, train_mape: {train_mape:.3f}, test_mse: {test_mse:.3f}, test_mape: {test_mape:.3f}\",flush=True)\n",
        "    # return results\n",
        "    loss_queue.put([train_engine.model.device, test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape])\n",
        "\n",
        "def sigchld_handler(signum, frame):\n",
        "    # adopt the exited child processes, collect results, identify avaliable devices\n",
        "    # since signal handler allows signal reentrancy, it may require atomic operation. but it's not safe to use lock in signal handler, refer to https://stackoverflow.com/questions/12445618/accessing-shared-data-from-a-signal-handler\n",
        "    # hence here ignore SIGCHLD signal before finishing cleaning up the process. note that new process should not be created before leaving sigchld_handler\n",
        "    # wait() will be always valid after child process status becomes exited unless pop out the process in config.procs_info['process_fd_table']\n",
        "    signal.signal(signal.SIGCHLD, signal.SIG_IGN)\n",
        "    while True:\n",
        "        p_sentns = wait(config.procs_info['process_fd_table'], 0)\n",
        "        if len(p_sentns) == 0:\n",
        "            break\n",
        "        # for each exited child process\n",
        "        for p_sentn in p_sentns:\n",
        "            # collect results\n",
        "            idx = config.procs_info['process_fd_table'].index(p_sentn)\n",
        "            # to prevent if one of the child process unexpectedly is killed and hence no results can be push into the queue, the parent process will be blocked in this line\n",
        "            try:\n",
        "                rank, test_date, *losses = config.procs_info['loss_queues'][idx].get(block=False)  # \"hang\" & Remove and return an item from the queue. If optional args block is True (the default) and timeout is None (the default), \"block\" if necessary until an item is available.\n",
        "            except queue.Empty:\n",
        "                print(\"at least one of the child process crashed, exit!\")\n",
        "                os.kill(os.getpid(), signal.SIGINT)\n",
        "            for v, k in zip([test_date]+losses, config.model_loss_list_dict):\n",
        "                config.model_loss_list_dict[k] += [v]\n",
        "\n",
        "            # update process info\n",
        "            config.procs_info['process_list'].pop(idx)\n",
        "            config.procs_info['loss_queues'].pop(idx)\n",
        "            config.procs_info['process_fd_table'].pop(idx)\n",
        "\n",
        "            # update progress bar and print results\n",
        "            plot_model_avg_loss(config.result_folder, config.args.model_type, {\n",
        "                k: [l for _, l in sorted(zip(config.model_loss_list_dict['date_list'], config.model_loss_list_dict[k]))]\n",
        "                for k in config.model_loss_list_dict.keys()})\n",
        "\n",
        "            if config.procs_info['fin_count'] % config.args.n_parallel_process == (config.args.n_parallel_process - 1) and config.procs_info['fin_count'] != 0:\n",
        "                config.progress_bar.update()\n",
        "            config.procs_info['fin_count'] = config.procs_info['fin_count'] + 1\n",
        "\n",
        "            # identify avaliable deiveces to allocate new process onto the rank which the exited process has stayed\n",
        "            config.procs_info['avaliable_device'].append(rank)\n",
        "    signal.signal(signal.SIGCHLD, sigchld_handler) # remember to reset the handler again, since we don't want to actually ignore SIGCHLD signal afterwards\n",
        "\n",
        "def sigint_handler(signum, frame):\n",
        "    # ctrl+c by user\n",
        "    # clear results in queues and terminate all the child process immediately\n",
        "    signal.signal(signal.SIGCHLD, signal.SIG_IGN)\n",
        "    print(\"catch KeyboardInterrupt, terminate all the child processes ...\")\n",
        "    for p, q in zip(config.procs_info['process_list'], config.procs_info['loss_queues']):\n",
        "        try:\n",
        "            _ = q.get(\n",
        "                block=False)  # Warning If a process is killed using Process.terminate() or os.kill() while it is trying to use a Queue, then the data in the queue is likely to become corrupted. This may cause any other process to get an exception when it tries to use the queue later on.\n",
        "        except queue.Empty:\n",
        "            pass  # fine\n",
        "        finally:\n",
        "            p.terminate()  # send SIGTERM signal to the child process\n",
        "            p.join()  # \"hang\" and adopt the child process\n",
        "            print(f\"child process {p.pid} terminated.\")\n",
        "    sys.exit(0)\n",
        "    # original_sigint_handler = signal.getsignal(signal.SIGINT)\n",
        "    # original_sigint_handler()\n",
        "\n",
        "def multi_running_worker(test_date_list):\n",
        "    # TODO: run on multiple GPUs: check the bugs related to torch.distributed.init_process_group & torch.nn.parallel.DistributedDataParallel\n",
        "    config.progress_bar = tqdm(test_date_list[::config.args.n_parallel_process])\n",
        "    config.progress_bar.reset()\n",
        "    signal.signal(signal.SIGCHLD, sigchld_handler)\n",
        "    signal.signal(signal.SIGINT, sigint_handler)\n",
        "\n",
        "    # initialize\n",
        "    for i in range(config.args.n_parallel_process):\n",
        "        multi_running_check_in(rank=i % torch.cuda.device_count() if torch.cuda.is_available() and config.args.on_gpu else -1)\n",
        "\n",
        "    while True:\n",
        "        if config.procs_info['EOF'] and len(config.procs_info['process_list']) == 0: # both conditions should be met\n",
        "            config.progress_bar.update()\n",
        "            break\n",
        "        elif len(config.procs_info['process_list']) < config.args.n_parallel_process and len(config.procs_info['avaliable_device']) > 0:\n",
        "            multi_running_check_in(rank=config.procs_info['avaliable_device'].pop(0))\n",
        "        else:\n",
        "            # suspend until receiving a signal when there still exist at least one process\n",
        "            signal.pause()\n",
        "    config.progress_bar.close()\n",
        "\n",
        "def main():\n",
        "    # ----------Hyper Parameters----------#\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument('--result_folder_tag', type=str, default='test')\n",
        "    parser.add_argument('--on_gpu', action='store_true')\n",
        "    parser.add_argument('--resume', action='store_true')\n",
        "    parser.add_argument('--batch_size', type=int, default=32)\n",
        "    parser.add_argument('--use_step_lr', action='store_true') # 調scheduler用，沒開是cyclic，打開是ms step\n",
        "    parser.add_argument('--num_workers', type=int, default=0)\n",
        "    # parser.add_argument('--window_size', type=int, default=5)\n",
        "    # 若window size為1時，在執行程式抓training set會讓rolling train day start跟end在同一天\n",
        "    parser.add_argument('--window_size', type=int, default=5)\n",
        "    parser.add_argument('--raw_dataset_path', type=str, default='.')\n",
        "    # parser.add_argument('--seed_list', type=int, nargs='+', default=[0, 1])\n",
        "    parser.add_argument('--seed_list', type=int, nargs='+', default=[0, 1])\n",
        "    # parser.add_argument('--sample_start_date', type=str, default='19960101')\n",
        "    parser.add_argument('--sample_start_date', type=str, default='20200316')\n",
        "    # parser.add_argument('--sample_times', type=int, default=36)\n",
        "    parser.add_argument('--sample_times', type=int, default=1)\n",
        "    # parser.add_argument('--sample_interval', type=int, default=10)\n",
        "    parser.add_argument('--sample_interval', type=int, default=1)\n",
        "    parser.add_argument('--n_parallel_process', type=int, default=1)\n",
        "    parser.add_argument('--model_type', type=str, default=\"multi\", choices=['multi', 'single'])\n",
        "    parser.add_argument('--daily_plot_off', action='store_true')\n",
        "    parser.add_argument('--residual_on', action='store_true')\n",
        "    parser.add_argument('--converge_delta', type=float, default=1e-5)\n",
        "    parser.add_argument('--converge_patience', type=int, default=300)\n",
        "    parser.add_argument('--master_addr', type=str, default='localhost') #主機的 IP\n",
        "    parser.add_argument('--master_port', type=str, default='16235') #主控機器開了一扇門（port）\n",
        "    # formula of rank\n",
        "    config.args = parser.parse_args(['--on_gpu', '--residual_on', '--daily_plot_off','--sample_start_date','19960101'])\n",
        "    #config.args = parser.parse_args()\n",
        "\n",
        "    # config.args.residual_on = True\n",
        "    # config.args.use_step_lr = True\n",
        "    print(f'on_gpu:{config.args.on_gpu},CV:{config.args.residual_on},start date:{config.args.sample_start_date},root:{config.args.raw_dataset_path}')\n",
        "    config.result_folder = f\"../20200316_{config.args.batch_size}batchsize\"\n",
        "    if config.args.use_step_lr:\n",
        "        scheduler_setting = config.ms_step_scheduler_setting\n",
        "        config.result_folder += f\"_{'_'.join(str(lr) for lr in scheduler_setting['init_lr_list'])}initLR_{'_'.join(str(w) for w in scheduler_setting['epoch_warmup_list'])}warmup_{scheduler_setting['epoch_decay']}decaylen_{scheduler_setting['decay_times']}decaytimes_{scheduler_setting['epoch_last']}epochlast\"\n",
        "    else: # cyclical lr\n",
        "        scheduler_setting = config.cyclic_shceduler_setting\n",
        "        config.result_folder += f\"_autoLR_autoCyclen_{scheduler_setting['cur_epoch_size']}epochsize_{scheduler_setting['lr_max_to_base_ratio']}MtoBratio\"\n",
        "    config.result_folder += f\"_{scheduler_setting['gamma']}gamma_{'_'.join(str(s) for s in config.args.seed_list)}seed\"\n",
        "    config.result_folder += '_residual' if config.args.residual_on else '_multi'\n",
        "    config.result_folder += f'_{config.args.result_folder_tag}_test'\n",
        "\n",
        "    clear_files(config.args.resume, config.args.sample_start_date, config.args.model_type, config.result_folder)\n",
        "\n",
        "    print(\"To reproduce AAAI Gated-neural-networks-for-option-pricing paper results\")\n",
        "    print(config.result_folder)\n",
        "\n",
        "    # ----------Initialization---------- #\n",
        "\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    os.environ['MASTER_ADDR'] = config.args.master_addr\n",
        "    os.environ['MASTER_PORT'] = config.args.master_port\n",
        "    data_processor = DataProcessor(root_path=config.args.raw_dataset_path, model_type='multi', sample_start_date=config.args.sample_start_date, sample_times=config.args.sample_times, sample_interval=config.args.sample_interval, on_gpu=config.args.on_gpu)\n",
        "    test_date_list, config.daily_gen, IAO_threshold = data_processor(config.args.window_size, config.args.batch_size, config.args.num_workers, config.args.n_parallel_process, config.args.residual_on, config.args.seed_list)\n",
        "    config.train_engine = TrainEngine(config.args.daily_plot_off, config.args.residual_on, config.args.use_step_lr, scheduler_setting, config.args.converge_delta, config.args.converge_patience, config.args.seed_list, IAO_threshold, config.result_folder)\n",
        "\n",
        "    # ----------For each testing date---------- #\n",
        "    if config.args.n_parallel_process == 1:\n",
        "        single_running_worker()\n",
        "\n",
        "    else:\n",
        "        multi_running_worker(test_date_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import train\n",
        "importlib.reload(train)\n",
        "\n",
        "# 然後再呼叫 train.train_engine(...) 或其他函數\n"
      ],
      "metadata": {
        "id": "pWyMue_izJiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654d86e4-8fd5-44e5-cc53-8977c1ae47df"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'train' from '/content/train.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "upoaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "9T4lk02okzex",
        "outputId": "572adf92-dcf3-44f6-9544-aa4dbf35ab24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ea4aed97-8476-47ec-b8cf-4008c6ccdf9a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ea4aed97-8476-47ec-b8cf-4008c6ccdf9a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving options_1996_2021_1.csv to options_1996_2021_1.csv\n"
          ]
        }
      ]
    }
  ]
}