{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMT4WSgexYz2yOf/qIHrGB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuki020936/Option-NN/blob/main/YUKI_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip install -r requirement.txt"
      ],
      "metadata": {
        "id": "HpjWROXZbcEV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "rJ-Nw6HBRyQe",
        "outputId": "d2505545-0cff-4687-8054-b9625b761a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on_gpu:True,CV:True,start date:19960101,root:.,num_workers:4,n_parallel_process:1\n",
            "To reproduce AAAI Gated-neural-networks-for-option-pricing paper results\n",
            "../19960101_32batchsize_autoLR_autoCyclen_1500epochsize_0.0625MtoBratio_0.99994gamma_0_1seed_residual_test_test\n",
            "True\n",
            "True\n",
            "epoch 100, train_loss: 252.780, train_mse: 8996418.786, train_mape: 37.776, test_loss:2393.175, test_mse: 12358508.273, test_mape: 46.612\n",
            "epoch 200, train_loss: 261.634, train_mse: 11346283.133, train_mape: 38.910, test_loss:1290.032, test_mse: 16374424.177, test_mape: 52.918\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c4e548a13ba3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-c4e548a13ba3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;31m# ----------For each testing date---------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_parallel_process\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msingle_running_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c4e548a13ba3>\u001b[0m in \u001b[0;36msingle_running_worker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# run model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtest_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ITMmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ITMmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_OTMmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_OTMmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ITMmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ITMmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_OTMmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_OTMmape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaily_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"date: {test_date}, train_mse: {train_mse:.3f}, train_mape: {train_mape:.3f}, test_mse: {test_mse:.3f}, test_mape: {test_mape:.3f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# record and update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, test_date, modell, train_dataloader_ITM_part, train_dataloader_OTM_part, train_dataloader, val_dataloader_ITM_part, val_dataloader_OTM_part, val_dataloader, test_dataloader_ITM_part, test_dataloader_OTM_part, test_dataloader, pdf_dataloader)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# 整理出一個可以用invm和tau進行索引，包含了invm及loss weight的表，其中不存在索引值相同的數據(invm和tau完全相同之值)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_loss_weight_for_residual_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader_ITM_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader_OTM_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mitm_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader_ITM_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader_ITM_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader_ITM_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_model_call_itm_part\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0motm_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader_OTM_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader_OTM_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader_OTM_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_model_call_itm_part\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mitm_loss_curve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.result_root_path}/test/itm_part/loss_message_{self.test_date}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36m_grid_search\u001b[0;34m(self, modell, train_dataloader, val_dataloader, test_dataloader, residual_model_call_itm_part)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCyclicLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcycle_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_range'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscdl_setting\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_momentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# self.scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=self.optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=cycle_len, mode='triangular2', cycle_momentum=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_the_best_param_and_corres_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_suffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mbest_loss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'min_val_metric'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the key set and order in dict must be: test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36m_update_the_best_param_and_corres_performance\u001b[0;34m(self, folder_suffix, seed_idx, best_loss_dict)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_val_metric'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mbest_loss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_val_metric'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;31m# if you observe that train loss becomes large abruptly, it may because of mape's denominator's 1e-8; btw, without 1e-8, denominator will be 0 and cause mape -> oo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_lr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36m_train_one_epoch\u001b[0;34m(self, running_lr_list)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_batch_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_one_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mone_batch_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# because dataloader will shuffle, current_loss's corresponding data is different for each time we reach the last times of the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_step_lr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# cyclical scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36m_train_one_iteration\u001b[0;34m(self, call_true, K, tau, S, r, d, is_syn)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pdb #debug\n",
        "import numpy as np\n",
        "import os\n",
        "from time import sleep #sleep(?)暫停?秒\n",
        "import signal #處理訊號（signal），這些訊號是作業系統在特定情況下發送給程式的事件\n",
        "import sys\n",
        "import queue #list(q.queue)\n",
        "import shutil\n",
        "import psutil\n",
        "from multiprocessing.connection import wait\n",
        "import torch.multiprocessing as mp\n",
        "import torch\n",
        "from argparse import ArgumentParser\n",
        "from dataset import DataProcessor\n",
        "from utils import clear_files\n",
        "from visulization import plot_model_avg_loss\n",
        "import config\n",
        "from train import TrainEngine\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import time\n",
        "\n",
        "#torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "def single_running_worker():\n",
        "    try:\n",
        "        while True:\n",
        "            # run model\n",
        "            test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape = config.train_engine(*next(config.daily_gen)(0))\n",
        "            print(f\"date: {test_date}, train_mse: {train_mse:.3f}, train_mape: {train_mape:.3f}, test_mse: {test_mse:.3f}, test_mape: {test_mape:.3f}\",flush=True)\n",
        "            # record and update\n",
        "            for v, k in zip([test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape], config.model_loss_list_dict):\n",
        "                config.model_loss_list_dict[k] += [v]\n",
        "            # plot_model_avg_loss(config.result_folder, config.args.model_type, config.model_loss_list_dict)\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            hours = int(elapsed_time // 3600)\n",
        "            minutes = int((elapsed_time % 3600) // 60)\n",
        "            seconds = int(elapsed_time % 60)\n",
        "            print(f\"執行時間：{hours:02d}小時 {minutes:02d}分 {seconds:02d}秒\")\n",
        "    except StopIteration:\n",
        "        pass  # EOF of generator, fine\n",
        "\n",
        "def multi_running_check_in(rank):\n",
        "    # rank is to allocate particular device\n",
        "    # torch.distributed.init_process_group(\"nccl\", rank=rank, world_size=torch.cuda.device_count()))\n",
        "    try:\n",
        "        q = mp.Queue()\n",
        "        p = mp.Process(target=multi_running_job, args=(next(config.daily_gen)(rank), q, config.train_engine))\n",
        "    except StopIteration:\n",
        "        config.procs_info['EOF'] = True\n",
        "        return # do nothing. that is, no new process will be allocated afterwards.\n",
        "\n",
        "    # if we don't update these process info before create new child process, parent process may receive SIGCHLD signal while config.procs_info['process_fd_table'] does not include this new process. hence parent process fail to wait this child process. however, it (child process exits immediately) rarely happens.\n",
        "    p.start()\n",
        "    config.procs_info['loss_queues'].append(q)\n",
        "    config.procs_info['process_list'].append(p)\n",
        "    config.procs_info['process_fd_table'].append(p.sentinel)\n",
        "\n",
        "def multi_running_job(daily_materials, loss_queue, train_engine):\n",
        "    # register signal as we hope SIGINT signal can only be processed by parent process\n",
        "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
        "    # run model\n",
        "    test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape = train_engine(*daily_materials)\n",
        "    print(f\"[process {os.getpid()}] date: {test_date}, train_mse: {train_mse:.3f}, train_mape: {train_mape:.3f}, test_mse: {test_mse:.3f}, test_mape: {test_mape:.3f}\",flush=True)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    hours = int(elapsed_time // 3600)\n",
        "    minutes = int((elapsed_time % 3600) // 60)\n",
        "    seconds = int(elapsed_time % 60)\n",
        "    print(f\"執行時間：{hours:02d}小時 {minutes:02d}分 {seconds:02d}秒\")\n",
        "    # return results\n",
        "    loss_queue.put([train_engine.model.device, test_date, train_loss, train_mse, train_mape, train_ITMmse, train_ITMmape, train_OTMmse, train_OTMmape, test_loss, test_mse, test_mape, test_ITMmse, test_ITMmape, test_OTMmse, test_OTMmape])\n",
        "\n",
        "def sigchld_handler(signum, frame):\n",
        "    # adopt the exited child processes, collect results, identify avaliable devices\n",
        "    # since signal handler allows signal reentrancy, it may require atomic operation. but it's not safe to use lock in signal handler, refer to https://stackoverflow.com/questions/12445618/accessing-shared-data-from-a-signal-handler\n",
        "    # hence here ignore SIGCHLD signal before finishing cleaning up the process. note that new process should not be created before leaving sigchld_handler\n",
        "    # wait() will be always valid after child process status becomes exited unless pop out the process in config.procs_info['process_fd_table']\n",
        "    signal.signal(signal.SIGCHLD, signal.SIG_IGN)\n",
        "    while True:\n",
        "        p_sentns = wait(config.procs_info['process_fd_table'], 0)\n",
        "        if len(p_sentns) == 0:\n",
        "            break\n",
        "        # for each exited child process\n",
        "        for p_sentn in p_sentns:\n",
        "            # collect results\n",
        "            idx = config.procs_info['process_fd_table'].index(p_sentn)\n",
        "            # to prevent if one of the child process unexpectedly is killed and hence no results can be push into the queue, the parent process will be blocked in this line\n",
        "            try:\n",
        "                rank, test_date, *losses = config.procs_info['loss_queues'][idx].get(block=False)  # \"hang\" & Remove and return an item from the queue. If optional args block is True (the default) and timeout is None (the default), \"block\" if necessary until an item is available.\n",
        "            except queue.Empty:\n",
        "                print(\"at least one of the child process crashed, exit!\")\n",
        "                os.kill(os.getpid(), signal.SIGINT)\n",
        "            for v, k in zip([test_date]+losses, config.model_loss_list_dict):\n",
        "                config.model_loss_list_dict[k] += [v]\n",
        "\n",
        "            # update process info\n",
        "            config.procs_info['process_list'].pop(idx)\n",
        "            config.procs_info['loss_queues'].pop(idx)\n",
        "            config.procs_info['process_fd_table'].pop(idx)\n",
        "\n",
        "            # update progress bar and print results\n",
        "            plot_model_avg_loss(config.result_folder, config.args.model_type, {\n",
        "                k: [l for _, l in sorted(zip(config.model_loss_list_dict['date_list'], config.model_loss_list_dict[k]))]\n",
        "                for k in config.model_loss_list_dict.keys()})\n",
        "\n",
        "            if config.procs_info['fin_count'] % config.args.n_parallel_process == (config.args.n_parallel_process - 1) and config.procs_info['fin_count'] != 0:\n",
        "                config.progress_bar.update()\n",
        "            config.procs_info['fin_count'] = config.procs_info['fin_count'] + 1\n",
        "\n",
        "            # identify avaliable deiveces to allocate new process onto the rank which the exited process has stayed\n",
        "            config.procs_info['avaliable_device'].append(rank)\n",
        "    signal.signal(signal.SIGCHLD, sigchld_handler) # remember to reset the handler again, since we don't want to actually ignore SIGCHLD signal afterwards\n",
        "\n",
        "def sigint_handler(signum, frame):\n",
        "    # ctrl+c by user\n",
        "    # clear results in queues and terminate all the child process immediately\n",
        "    signal.signal(signal.SIGCHLD, signal.SIG_IGN)\n",
        "    print(\"catch KeyboardInterrupt, terminate all the child processes ...\")\n",
        "    for p, q in zip(config.procs_info['process_list'], config.procs_info['loss_queues']):\n",
        "        try:\n",
        "            _ = q.get(\n",
        "                block=False)  # Warning If a process is killed using Process.terminate() or os.kill() while it is trying to use a Queue, then the data in the queue is likely to become corrupted. This may cause any other process to get an exception when it tries to use the queue later on.\n",
        "        except queue.Empty:\n",
        "            pass  # fine\n",
        "        finally:\n",
        "            p.terminate()  # send SIGTERM signal to the child process\n",
        "            p.join()  # \"hang\" and adopt the child process\n",
        "            print(f\"child process {p.pid} terminated.\")\n",
        "    sys.exit(0)\n",
        "    # original_sigint_handler = signal.getsignal(signal.SIGINT)\n",
        "    # original_sigint_handler()\n",
        "\n",
        "def multi_running_worker(test_date_list):\n",
        "    # TODO: run on multiple GPUs: check the bugs related to torch.distributed.init_process_group & torch.nn.parallel.DistributedDataParallel\n",
        "    config.progress_bar = tqdm(test_date_list[::config.args.n_parallel_process])\n",
        "    config.progress_bar.reset()\n",
        "    signal.signal(signal.SIGCHLD, sigchld_handler)\n",
        "    signal.signal(signal.SIGINT, sigint_handler)\n",
        "\n",
        "    # initialize\n",
        "    for i in range(config.args.n_parallel_process):\n",
        "        multi_running_check_in(rank=i % torch.cuda.device_count() if torch.cuda.is_available() and config.args.on_gpu else -1)\n",
        "\n",
        "    while True:\n",
        "        if config.procs_info['EOF'] and len(config.procs_info['process_list']) == 0: # both conditions should be met\n",
        "            config.progress_bar.update()\n",
        "            break\n",
        "        elif len(config.procs_info['process_list']) < config.args.n_parallel_process and len(config.procs_info['avaliable_device']) > 0:\n",
        "            multi_running_check_in(rank=config.procs_info['avaliable_device'].pop(0))\n",
        "        else:\n",
        "            # suspend until receiving a signal when there still exist at least one process\n",
        "            signal.pause()\n",
        "    config.progress_bar.close()\n",
        "\n",
        "def main(start_time):\n",
        "    # ----------Hyper Parameters----------#\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument('--result_folder_tag', type=str, default='test')\n",
        "    parser.add_argument('--on_gpu', action='store_true')\n",
        "    parser.add_argument('--resume', action='store_true')\n",
        "    parser.add_argument('--batch_size', type=int, default=32)\n",
        "    parser.add_argument('--use_step_lr', action='store_true') # 調scheduler用，沒開是cyclic，打開是ms step\n",
        "    parser.add_argument('--num_workers', type=int, default=0)\n",
        "    # parser.add_argument('--window_size', type=int, default=5)\n",
        "    # 若window size為1時，在執行程式抓training set會讓rolling train day start跟end在同一天\n",
        "    parser.add_argument('--window_size', type=int, default=5)\n",
        "    parser.add_argument('--raw_dataset_path', type=str, default='.')\n",
        "    # parser.add_argument('--seed_list', type=int, nargs='+', default=[0, 1])\n",
        "    parser.add_argument('--seed_list', type=int, nargs='+', default=[0, 1])\n",
        "    # parser.add_argument('--sample_start_date', type=str, default='19960101')\n",
        "    parser.add_argument('--sample_start_date', type=str, default='20200316')\n",
        "    # parser.add_argument('--sample_times', type=int, default=36)\n",
        "    parser.add_argument('--sample_times', type=int, default=1)\n",
        "    # parser.add_argument('--sample_interval', type=int, default=10)\n",
        "    parser.add_argument('--sample_interval', type=int, default=1)\n",
        "    parser.add_argument('--n_parallel_process', type=int, default=1)\n",
        "    parser.add_argument('--model_type', type=str, default=\"multi\", choices=['multi', 'single'])\n",
        "    parser.add_argument('--daily_plot_off', action='store_true')\n",
        "    parser.add_argument('--residual_on', action='store_true')\n",
        "    parser.add_argument('--converge_delta', type=float, default=1e-5)\n",
        "    parser.add_argument('--converge_patience', type=int, default=300)\n",
        "    parser.add_argument('--master_addr', type=str, default='localhost') #主機的 IP\n",
        "    parser.add_argument('--master_port', type=str, default='16235') #主控機器開了一扇門（port）\n",
        "    # formula of rank\n",
        "    config.args = parser.parse_args(['--on_gpu', '--residual_on', '--daily_plot_off','--sample_start_date','19960101','--num_workers','4'])\n",
        "    #config.args = parser.parse_args()\n",
        "\n",
        "    # config.args.residual_on = True\n",
        "    # config.args.use_step_lr = True\n",
        "    print(f'on_gpu:{config.args.on_gpu},CV:{config.args.residual_on},start date:{config.args.sample_start_date},root:{config.args.raw_dataset_path},num_workers:{config.args.num_workers},n_parallel_process:{config.args.n_parallel_process}')\n",
        "    config.result_folder = f\"../{config.args.sample_start_date}_{config.args.batch_size}batchsize\"\n",
        "    if config.args.use_step_lr:\n",
        "        scheduler_setting = config.ms_step_scheduler_setting\n",
        "        config.result_folder += f\"_{'_'.join(str(lr) for lr in scheduler_setting['init_lr_list'])}initLR_{'_'.join(str(w) for w in scheduler_setting['epoch_warmup_list'])}warmup_{scheduler_setting['epoch_decay']}decaylen_{scheduler_setting['decay_times']}decaytimes_{scheduler_setting['epoch_last']}epochlast\"\n",
        "    else: # cyclical lr\n",
        "        scheduler_setting = config.cyclic_shceduler_setting\n",
        "        config.result_folder += f\"_autoLR_autoCyclen_{scheduler_setting['cur_epoch_size']}epochsize_{scheduler_setting['lr_max_to_base_ratio']}MtoBratio\"\n",
        "    config.result_folder += f\"_{scheduler_setting['gamma']}gamma_{'_'.join(str(s) for s in config.args.seed_list)}seed\"\n",
        "    config.result_folder += '_residual' if config.args.residual_on else '_multi'\n",
        "    config.result_folder += f'_{config.args.result_folder_tag}_test'\n",
        "\n",
        "    clear_files(config.args.resume, config.args.sample_start_date, config.args.model_type, config.result_folder)\n",
        "\n",
        "    print(\"To reproduce AAAI Gated-neural-networks-for-option-pricing paper results\")\n",
        "    print(config.result_folder)\n",
        "\n",
        "    # ----------Initialization---------- #\n",
        "\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    os.environ['MASTER_ADDR'] = config.args.master_addr\n",
        "    os.environ['MASTER_PORT'] = config.args.master_port\n",
        "    data_processor = DataProcessor(root_path=config.args.raw_dataset_path, model_type='multi', sample_start_date=config.args.sample_start_date, sample_times=config.args.sample_times, sample_interval=config.args.sample_interval, on_gpu=config.args.on_gpu)\n",
        "    test_date_list, config.daily_gen, IAO_threshold = data_processor(config.args.window_size, config.args.batch_size, config.args.num_workers, config.args.n_parallel_process, config.args.residual_on, config.args.seed_list)\n",
        "    config.train_engine = TrainEngine(config.args.daily_plot_off, config.args.residual_on, config.args.use_step_lr, scheduler_setting, config.args.converge_delta, config.args.converge_patience, config.args.seed_list, IAO_threshold, config.result_folder,start_time)\n",
        "\n",
        "    # ----------For each testing date---------- #\n",
        "    if config.args.n_parallel_process == 1:\n",
        "        single_running_worker()\n",
        "\n",
        "    else:\n",
        "        multi_running_worker(test_date_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    main(start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import train\n",
        "import visulization\n",
        "importlib.reload(train)\n",
        "importlib.reload(visulization)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T4lk02okzex",
        "outputId": "3c2bf344-0224-4377-800a-44fbb0ff9b17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'visulization' from '/content/visulization.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f'cpu核心數:{os.cpu_count()}')\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "def suggest_optimal_workers():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    cpu_count = os.cpu_count()\n",
        "    total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
        "\n",
        "    # 推估 num_workers：根據 CPU 和 RAM\n",
        "    if total_ram_gb >= 16 and cpu_count >= 4:\n",
        "        num_workers = min(4, cpu_count // 2)\n",
        "    elif total_ram_gb >= 8:\n",
        "        num_workers = min(2, cpu_count // 2)\n",
        "    else:\n",
        "        num_workers = 0\n",
        "\n",
        "    # 推估 n_parallel_process：根據 GPU 數量\n",
        "    if gpu_count >= 1:\n",
        "        n_parallel_process = 1  # Colab A100 為單卡，建議保守設定為 1\n",
        "    else:\n",
        "        n_parallel_process = min(2, cpu_count // 2)  # 無 GPU 時可開多進程（風險較高）\n",
        "\n",
        "    return {\n",
        "        \"gpu_count\": gpu_count,\n",
        "        \"cpu_count\": cpu_count,\n",
        "        \"total_ram_gb\": round(total_ram_gb, 2),\n",
        "        \"suggested_num_workers\": num_workers,\n",
        "        \"suggested_n_parallel_process\": n_parallel_process\n",
        "    }\n",
        "\n",
        "# 執行並印出建議\n",
        "suggestion = suggest_optimal_workers()\n",
        "for k, v in suggestion.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx2IJoFYZCcK",
        "outputId": "6d7ffc95-3a9f-419c-9bc0-84d8ce0879e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu核心數:12\n",
            "gpu_count: 1\n",
            "cpu_count: 12\n",
            "total_ram_gb: 83.48\n",
            "suggested_num_workers: 4\n",
            "suggested_n_parallel_process: 1\n"
          ]
        }
      ]
    }
  ]
}